{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10003 episode transcripts in the sample.\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"SELECT transcript FROM podcast_episodes\"\n",
    "cursor.execute(query)\n",
    "episodes = cursor.fetchall()\n",
    "\n",
    "print(f\"There are {len(episodes)} episode transcripts in the sample.\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicating LDA with parameters used in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization error for episode ID 6660: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and clean text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [re.sub(r\"[^a-z]\", \"\", token) for token in tokens]  # Keep only alphabets\n",
    "    tokens = [token for token in tokens if token and token not in stop_words and len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Connect to database\n",
    "db_path = \"data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Fetch all records from the table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT rowid, transcript FROM podcast_episodes\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Process transcripts to extract the first 1000 words\n",
    "processed_data = []\n",
    "texts = []\n",
    "id_mapping = []\n",
    "\n",
    "for row in rows:\n",
    "    episode_id, transcript = row\n",
    "    if transcript and isinstance(transcript, str) and transcript.strip():\n",
    "        try:\n",
    "            words = preprocess_text(\" \".join(word_tokenize(transcript)[:1000]))\n",
    "            transcript_1000 = \" \".join(words)\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error for episode ID {episode_id}: {e}\")\n",
    "            transcript_1000 = \"\"\n",
    "    else:\n",
    "        transcript_1000 = \"\"\n",
    "    \n",
    "    processed_data.append((transcript_1000, episode_id))\n",
    "    texts.append(words if transcript_1000 else [])\n",
    "    id_mapping.append(episode_id)\n",
    "\n",
    "# Update transcript_1000 in the database\n",
    "cursor.executemany(\"UPDATE podcast_episodes SET transcript_1000_tokens = ? WHERE rowid = ?\", processed_data)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Build dictionary and corpus for LDA\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database updated with topic modeling 1000 results.\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "db_path = \"data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Fetch all records from the table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT rowid FROM podcast_episodes\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Set Gensim LDA parameters similar to MALLET\n",
    "num_topics = 200\n",
    "passes = 10  # Rough equivalent to optimize_interval\n",
    "workers = 50  # Multi-threading for parallel processing\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Prepare topic data for each episode\n",
    "update_data = []\n",
    "for i, bow in enumerate(corpus):\n",
    "    episode_id = id_mapping[i]\n",
    "    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n",
    "\n",
    "    # Convert probabilities to standard Python float\n",
    "    dist_dict = {int(topic_id): float(prob) for topic_id, prob in topic_dist}\n",
    "    \n",
    "    # JSON serialization\n",
    "    json_distribution = json.dumps(dist_dict)\n",
    "    \n",
    "    update_data.append((dominant_topic, json_distribution, episode_id))\n",
    "\n",
    "# Update the database with dominant_topic_number and topic_distribution\n",
    "cursor.executemany(\"UPDATE podcast_episodes SET dominant_topic_number_1000 = ?, topic_distribution_1000 = ? WHERE rowid = ?\", update_data)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database updated with topic modeling 1000 results.\")\n",
    "\n",
    "# Save outputs similar to MALLET\n",
    "lda_model.save(\"gensim_lda_model\")\n",
    "topics = lda_model.print_topics(num_topics=num_topics)\n",
    "with open(\"../data/doc_topics_1000.txt\", \"w\") as doc_topics_file:\n",
    "    for doc_num, topic_probs in enumerate(lda_model[corpus]):\n",
    "        doc_topics_file.write(f\"Document {doc_num}: {topic_probs}\\n\")\n",
    "\n",
    "with open(\"../data/topic_keys_1000.txt\", \"w\") as topic_keys_file:\n",
    "    for topic_num, topic_words in topics:\n",
    "        topic_keys_file.write(f\"Topic {topic_num}: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole transcript tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization error for episode ID 6660: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "db_path = \"data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Fetch all records from the table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT rowid, transcript FROM podcast_episodes\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Process transcripts to extract the first 1000 words\n",
    "processed_data = []\n",
    "texts = []\n",
    "id_mapping = []\n",
    "\n",
    "for row in rows:\n",
    "    episode_id, transcript = row\n",
    "    if transcript and isinstance(transcript, str) and transcript.strip():\n",
    "        try:\n",
    "            words = preprocess_text(\" \".join(word_tokenize(transcript)))\n",
    "            transcript = \" \".join(words)\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error for episode ID {episode_id}: {e}\")\n",
    "            transcript = \"\"\n",
    "    else:\n",
    "        transcript = \"\"\n",
    "    \n",
    "    processed_data.append((transcript, episode_id))\n",
    "    texts.append(words if transcript else [])\n",
    "    id_mapping.append(episode_id)\n",
    "\n",
    "# Update transcript_1000 in the database\n",
    "cursor.executemany(\"UPDATE podcast_episodes SET transcript_tokens = ? WHERE rowid = ?\", processed_data)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Build dictionary and corpus for LDA\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database updated with topic modeling results.\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "db_path = \"data.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Fetch all records from the table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT rowid FROM podcast_episodes\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Set Gensim LDA parameters similar to MALLET\n",
    "num_topics = 200\n",
    "passes = 10  # Rough equivalent to optimize_interval\n",
    "workers = 50  # Multi-threading for parallel processing\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "# num_topics = 200\n",
    "# lda_model = LdaModel(\n",
    "#     corpus=corpus,\n",
    "#     id2word=dictionary,\n",
    "#     num_topics=num_topics,\n",
    "#     passes=passes,  # Number of passes over the entire corpus (analogous to optimize interval)\n",
    "#     iterations=100,  # Optional: Set higher for better convergence\n",
    "#     random_state=42,\n",
    "#     chunksize=1000,\n",
    "#     alpha='auto',  # Automatic hyperparameter optimization\n",
    "#     eta='auto',\n",
    "#     eval_every=None  # Turn off frequent model evaluation for efficiency\n",
    "# )\n",
    "# Prepare topic data for each episode\n",
    "update_data = []\n",
    "for i, bow in enumerate(corpus):\n",
    "    episode_id = id_mapping[i]\n",
    "    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n",
    "\n",
    "    # Convert probabilities to standard Python float\n",
    "    dist_dict = {int(topic_id): float(prob) for topic_id, prob in topic_dist}\n",
    "    \n",
    "    # JSON serialization\n",
    "    json_distribution = json.dumps(dist_dict)\n",
    "    \n",
    "    update_data.append((dominant_topic, json_distribution, episode_id))\n",
    "\n",
    "# Update the database with dominant_topic_number and topic_distribution\n",
    "cursor.executemany(\"UPDATE podcast_episodes SET dominant_topic_number = ?, topic_distribution = ? WHERE rowid = ?\", update_data)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database updated with topic modeling results.\")\n",
    "\n",
    "# Save outputs similar to MALLET\n",
    "lda_model.save(\"gensim_lda_model\")\n",
    "topics = lda_model.print_topics(num_topics=num_topics)\n",
    "with open(\"../data/doc_topics.txt\", \"w\") as doc_topics_file:\n",
    "    for doc_num, topic_probs in enumerate(lda_model[corpus]):\n",
    "        doc_topics_file.write(f\"Document {doc_num}: {topic_probs}\\n\")\n",
    "\n",
    "with open(\"../data/topic_keys.txt\", \"w\") as topic_keys_file:\n",
    "    for topic_num, topic_words in topics:\n",
    "        topic_keys_file.write(f\"Topic {topic_num}: {topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MALLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlite3\n",
    "# import json\n",
    "# from gensim.models.wrappers import LdaMallet\n",
    "# from gensim.corpora import Dictionary\n",
    "# from gensim.models import CoherenceModel\n",
    "# import os\n",
    "\n",
    "# # Path to MALLET binary (update this path as necessary)\n",
    "# path_to_mallet_binary = \"~/Documents/GitHub/Mallet/\"\n",
    "\n",
    "# # Database connection setup\n",
    "# db_path = \"data.db\"\n",
    "# conn = sqlite3.connect(db_path)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# # Fetch text data from the database\n",
    "# cursor.execute(\"SELECT rowid, text_column FROM podcast_episodes\")  # Replace text_column with your column name\n",
    "# rows = cursor.fetchall()\n",
    "\n",
    "# # Prepare data for LDA\n",
    "# id_mapping = {}\n",
    "# documents = []\n",
    "# for rowid, text in rows:\n",
    "#     id_mapping[len(documents)] = rowid\n",
    "#     documents.append(text.split())  # Tokenize text; adjust as needed based on preprocessing\n",
    "\n",
    "# # Create a Gensim dictionary and corpus\n",
    "# dictionary = Dictionary(documents)\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# # Define LDA parameters\n",
    "# num_topics = 200\n",
    "# passes = 10\n",
    "\n",
    "# # Train LDA Mallet model\n",
    "# lda_mallet = LdaMallet(\n",
    "#     mallet_path,\n",
    "#     corpus=corpus,\n",
    "#     num_topics=num_topics,\n",
    "#     id2word=dictionary,\n",
    "#     optimize_interval=passes\n",
    "# )\n",
    "\n",
    "# # Extract topics for each document\n",
    "# update_data = []\n",
    "# for i, bow in enumerate(corpus):\n",
    "#     episode_id = id_mapping[i]\n",
    "#     topic_dist = lda_mallet[bow]\n",
    "#     dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n",
    "\n",
    "#     # Convert probabilities to standard Python float\n",
    "#     dist_dict = {int(topic_id): float(prob) for topic_id, prob in topic_dist}\n",
    "\n",
    "#     # JSON serialization\n",
    "#     json_distribution = json.dumps(dist_dict)\n",
    "\n",
    "#     update_data.append((dominant_topic, json_distribution, episode_id))\n",
    "\n",
    "# # Update the database\n",
    "# cursor.executemany(\"UPDATE podcast_episodes SET dominant_topic_number_mallet = ?, topic_distribution_mallet = ? WHERE rowid = ?\", update_data)\n",
    "# conn.commit()\n",
    "# conn.close()\n",
    "\n",
    "# print(\"Database updated with topic modeling results.\")\n",
    "\n",
    "# # Save model and topic outputs\n",
    "# lda_mallet.save(\"gensim_lda_mallet_model\")\n",
    "\n",
    "# # Save document topics\n",
    "# doc_topics_path = \"../data/doc_topic_mallet.txt\"\n",
    "# with open(doc_topics_path, \"w\") as doc_topics_file:\n",
    "#     for doc_num, topic_probs in enumerate(lda_mallet[corpus]):\n",
    "#         doc_topics_file.write(f\"Document {doc_num}: {topic_probs}\\n\")\n",
    "\n",
    "# # Save topic keys\n",
    "# topic_keys_path = \"../data/topic_keys_mallet.txt\"\n",
    "# topics = lda_mallet.show_topics(num_topics=num_topics, formatted=True)\n",
    "# with open(topic_keys_path, \"w\") as topic_keys_file:\n",
    "#     for topic_num, topic_words in topics:\n",
    "#         topic_keys_file.write(f\"Topic {topic_num}: {topic_words}\\n\")\n",
    "\n",
    "# print(\"Topic modeling outputs saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check racial justice topics episode count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with racial justice-related dominant_topic_number_1000: 127\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "conn = sqlite3.connect('data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the query to count the number of rows with dominant_topic_number = ?\n",
    "query = \"SELECT COUNT(*) FROM podcast_episodes WHERE dominant_topic_number_1000 = 6 OR dominant_topic_number_1000 = 110\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch the result\n",
    "count = cursor.fetchone()[0]\n",
    "print(f\"Number of rows with racial justice-related dominant_topic_number_1000: {count}\")\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in podcast_episodes: ['transcript', 'rssUrl', 'epTitle', 'epDescription', 'mp3url', 'podTitle', 'lastUpdate', 'itunesAuthor', 'itunesOwnerName', 'explicit', 'imageUrl', 'language', 'createdOn', 'host', 'podDescription', 'category1', 'category2', 'category3', 'category4', 'category5', 'category6', 'category7', 'category8', 'category9', 'category10', 'oldestEpisodeDate', 'episodeDateLocalized', 'durationSeconds', 'hostPredictedNames', 'numUniqueHosts', 'guestPredictedNames', 'numUniqueGuests', 'neitherPredictedNames', 'numUniqueNeithers', 'mainEpSpeakers', 'numMainSpeakers', 'hostSpeakerLabels', 'guestSpeakerLabels', 'overlapPropTurnCount', 'avgTurnDuration', 'overlapPropDuration', 'totalSpLabels', 'BLM', 'description_tokens', 'description_topic', 'transcript_tokens', 'transcript_topic', 'sentences', 'topic_distribution', 'transcript_1000_tokens', 'dominant_topic_number', 'dominant_topic_number_1000', 'topic_distribution_1000']\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "conn = sqlite3.connect('data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the query to get column names\n",
    "query = 'PRAGMA table_info(podcast_episodes)'\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch the result\n",
    "columns_info = cursor.fetchall()\n",
    "column_names = [info[1] for info in columns_info]\n",
    "print(f\"Column names in podcast_episodes: {column_names}\")\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
