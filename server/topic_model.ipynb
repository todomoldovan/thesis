{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran LDA model on the first 1000 words of each podcast episode transcript with 200 topics, using Mallet. Each episode is then represented as its topic distribution. Depicted using a sample of 25K episodes, colored by category, and projected using t-SNE on episodes’ topic distributions to visualize topical distance, with select topic clusters annotated using the top words in the corresponding topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '<p>Best of with snippets from 3 episodes and acoustic performance of 3 new songs.</p>', 'best snippets episodes acoustic performance new songs')\n",
      "(2, '<p>Simon introduces &apos;It&apos;s all Gone&apos;, track 4 of the newly released KisTone Album, &apos;Way To Nowhere&apos;.</p>', 'simon introduces apositaposs goneapos track newly released kistone album aposway nowhereapos')\n",
      "(3, '<p>Simon introduces track 4, of the newly released\\xa0 KisTone album,\\xa0 &apos;Today is Yesterday&apos;.</p>', 'simon introduces track newly released kistone album apostoday yesterdayapos')\n",
      "(4, '<p>This week, Simon introduces track 2- Saturn Return of the KisTone album, &apos;Way To Nowhere&apos; and describes the background and motivation in co-writing the song \\xa0with fellow band mates.</p>', 'week simon introduces track saturn return kistone album aposway nowhereapos describes background motivation cowriting song fellow band mates')\n",
      "(5, '<p>Big news week. The band Simon lived in the USA with, KisTone has released their debut album after 12 years of red tape. Simon runs through a brief history of the band and then introduces track 1, Quarterlife Crisis.</p>', 'big news week band simon lived usa kistone released debut album years red tape simon runs brief history band introduces track quarterlife crisis')\n",
      "(6, '<p>Listen as Educator Barnes explains how recent events have magnified the stress and anxiety of Black educators\\xa0\\xa0</p>\\r', 'listen educator barnes explains recent events magnified stress anxiety black educators')\n",
      "(7, '<p>Listen as Educator Barnes talks to Natalie Pipkin about homeschooling Black children.</p>\\r', 'listen educator barnes talks natalie pipkin homeschooling black childrenp')\n",
      "(8, 'It’s no secret that the COVID-19 pandemic has made a huge impact on higher education. In this podcast episode, Chancellor Eloy Ortiz Oakley is joined by Monica Lozano, CEO of the College Futures Foundation to discuss the challenges the California is facing due to COVID-19 and the efforts her organization is making to confront the equity gap that has existed in the system for far too long.Transcript:  https://www.cccco.edu/-/media/CCCCO-Website/Podcasts/Transcripts/CCC20033.pdf', 'secret covid pandemic made huge impact higher education podcast episode chancellor eloy ortiz oakley joined monica lozano ceo college futures foundation discuss challenges california facing due covid efforts organization making confront equity gap existed system far longtranscript httpswwwccccoedumediaccccowebsitepodcaststranscriptscccpdf')\n",
      "(9, \"The girls are back in town once again because the NHL cannot stop bungling their increasingly pathetic attempt to reenact the 2020 Stanley Cup Playoffs in Las Vegas. This week, 11 players tested positive (for corona, not cooties), Roman Polak yeeted from America, and the Sabres continue to embarass themselves. Also, we rank Elliotte Friedman's new beard.LINKS! LINKS! INFO!Outro: Figure Eight - Peach PitCheck out the Blue Wire survey here: https://bit.ly/2Ybr18VVisit BetOnline.AG and use promo code BLUEWIRE for a free welcome bonus!Learn more about your ad choices. Visit podcastchoices.com/adchoices\", 'girls back town nhl stop bungling increasingly pathetic attempt reenact stanley cup playoffs las vegas week players tested positive corona cooties roman polak yeeted america sabres continue embarass also rank elliotte friedmans new beardlinks links infooutro figure eight peach pitcheck blue wire survey httpsbitlyybrvvisit betonlineag use promo code bluewire free welcome bonuslearn choices visit podcastchoicescomadchoices')\n",
      "(10, '<p>When we experience the symptoms of anxiety such as the heart racing or feeling sweaty and shaky we are actually experiencing our bodily response to fear.</p><p>Our fear response comes about because our brains have evolved to keep us alive for as long as possible, his means that it Is hard wires to fear things that threaten our lives.</p><p>What can happen in people with anxiety is that the fear response doesn’t only react to life threatening situations, it starts to respond to situations where death is very unlikely.</p><p>An example of this is fear of flying, statistically the risk of death is low but people with a fear of flying experience sometimes very extreme symptoms when faced with even the thought of flying.</p><p>Find out in this episode what makes our fear response react to non-life threatening situations and what you can do to manage your anxiety.</p>\\r', 'experience symptoms anxiety heart racing feeling sweaty shaky actually experiencing bodily response fearppour fear response comes brains evolved keep alive long possible means hard wires fear things threaten livesppwhat happen people anxiety fear response doesnt react life threatening situations starts respond situations death unlikelyppan example fear flying statistically risk death low people fear flying experience sometimes extreme symptoms faced even thought flyingppfind episode makes fear response react nonlife threatening situations manage anxietyp')\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch the first 10 entries from podcast_episodes\n",
    "query = \"SELECT rowid, epDescription, tokens FROM podcast_episodes LIMIT 10\"\n",
    "cursor.execute(query)\n",
    "episodes = cursor.fetchall()\n",
    "\n",
    "# Print the first 10 entries\n",
    "for episode in episodes:\n",
    "    print(episode)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1124058 episode descriptions.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect('data.db')\n",
    "\n",
    "query = \"SELECT epDescription, category1, category2, category3, category4, category5, category6, category7, category8, category9, category10 FROM podcast_episodes WHERE epDescription IS NOT NULL\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Loaded {len(df)} episode descriptions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epDescription</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>category4</th>\n",
       "      <th>category5</th>\n",
       "      <th>category6</th>\n",
       "      <th>category7</th>\n",
       "      <th>category8</th>\n",
       "      <th>category9</th>\n",
       "      <th>category10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;p&gt;Best of with snippets from 3 episodes and a...</td>\n",
       "      <td>music</td>\n",
       "      <td>society</td>\n",
       "      <td>culture</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>education</td>\n",
       "      <td>self improvement</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;Simon introduces &amp;apos;It&amp;apos;s all Gone&amp;a...</td>\n",
       "      <td>music</td>\n",
       "      <td>society</td>\n",
       "      <td>culture</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>education</td>\n",
       "      <td>self improvement</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;p&gt;Simon introduces track 4, of the newly rele...</td>\n",
       "      <td>music</td>\n",
       "      <td>society</td>\n",
       "      <td>culture</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>education</td>\n",
       "      <td>self improvement</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;This week, Simon introduces track 2- Saturn...</td>\n",
       "      <td>music</td>\n",
       "      <td>society</td>\n",
       "      <td>culture</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>education</td>\n",
       "      <td>self improvement</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Big news week. The band Simon lived in the ...</td>\n",
       "      <td>music</td>\n",
       "      <td>society</td>\n",
       "      <td>culture</td>\n",
       "      <td>philosophy</td>\n",
       "      <td>education</td>\n",
       "      <td>self improvement</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       epDescription category1 category2  \\\n",
       "0  <p>Best of with snippets from 3 episodes and a...     music   society   \n",
       "1  <p>Simon introduces &apos;It&apos;s all Gone&a...     music   society   \n",
       "2  <p>Simon introduces track 4, of the newly rele...     music   society   \n",
       "3  <p>This week, Simon introduces track 2- Saturn...     music   society   \n",
       "4  <p>Big news week. The band Simon lived in the ...     music   society   \n",
       "\n",
       "  category3   category4  category5         category6 category7 category8  \\\n",
       "0   culture  philosophy  education  self improvement      NULL      NULL   \n",
       "1   culture  philosophy  education  self improvement      NULL      NULL   \n",
       "2   culture  philosophy  education  self improvement      NULL      NULL   \n",
       "3   culture  philosophy  education  self improvement      NULL      NULL   \n",
       "4   culture  philosophy  education  self improvement      NULL      NULL   \n",
       "\n",
       "  category9 category10  \n",
       "0      NULL       NULL  \n",
       "1      NULL       NULL  \n",
       "2      NULL       NULL  \n",
       "3      NULL       NULL  \n",
       "4      NULL       NULL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk_data_path = \"../data/nltk_data\"\n",
    "# nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# nltk.download('punkt', download_dir=nltk_data_path)\n",
    "# nltk.download('stopwords', download_dir=nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/cephyr/users/theale/Alvis/nltk_data'\n    - '/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/nltk_data'\n    - '/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/share/nltk_data'\n    - '/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbanana\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[1;32m     11\u001b[0m \u001b[39m# def preprocess_text(text):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m#     # Remove non-alphanumeric characters and lowercase\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m#     text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# # Apply preprocessing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# df['tokens'] = df['epDescription'].apply(preprocess_text)\u001b[39;00m\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[0;32m/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/cephyr/users/theale/Alvis/nltk_data'\n    - '/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/nltk_data'\n    - '/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/share/nltk_data'\n    - '/mimer/NOBACKUP/groups/naiss2024-22-185/theodora/nltk_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = \"banana\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Remove non-alphanumeric characters and lowercase\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "#     # Tokenize and remove stopwords\n",
    "#     tokens = word_tokenize(text)\n",
    "#     return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# # Apply preprocessing\n",
    "# df['tokens'] = df['epDescription'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 200\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Get topic distributions for each episode\n",
    "episode_topics = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "\n",
    "# Convert to dense representation for visualization\n",
    "import numpy as np\n",
    "dense_topics = np.zeros((len(episode_topics), num_topics))\n",
    "for i, topic_dist in enumerate(episode_topics):\n",
    "    for topic_id, topic_value in topic_dist:\n",
    "        dense_topics[i, topic_id] = topic_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_results = tsne.fit_transform(dense_topics)\n",
    "\n",
    "# Plot results with colors by category\n",
    "categories = df['category1'].fillna('Unknown')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=pd.factorize(categories)[0], cmap='tab20')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=pd.factorize(categories)[1])\n",
    "plt.title(\"t-SNE Visualization of Podcast Topics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top words for each topic\n",
    "for i in range(num_topics):\n",
    "    words = lda_model.show_topic(i, topn=10)\n",
    "    top_words = \", \".join([word for word, _ in words])\n",
    "    print(f\"Topic {i}: {top_words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3631e189d5f8ed7dd4c171108afc0e902f6a517eb50e8966ab80904756faf8c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
